---
title: "Class 3 - ML in Genomics: Clustering"
subtitle: "An introduction to clustering methods"
author: Renan Escalante
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "assets/css/my-theme.css", "assets/css/my-fonts.css"]
    seal: false 
    lib_dir: libs
    nature:
      highlightStyle: solarized-light
      highlightLanguage: ["r", "css", "yaml"]
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(collapse = TRUE,
                      message = FALSE, 
                      warning = FALSE, 
                      fig.align = TRUE, 
                      cache = TRUE, 
                      fig.retina = 3)

library(tidyverse)
library(tidymodels)
library(tidyclust)
library(palmerpenguins)
library(factoextra)
library(ggforce)
library(xaringanExtra)
xaringanExtra::use_panelset()
xaringanExtra::use_share_again()
xaringanExtra::style_share_again(
  share_buttons = c("twitter", "linkedin", "pocket")
)
```

```{r broadcast, echo=FALSE}
xaringanExtra::use_broadcast()
```

class: title-slide, center

# `r rmarkdown::metadata$title`

## `r rmarkdown::metadata$author`

### `r rmarkdown::metadata$subtitle`
### `r rmarkdown::metadata$date`

---

layout: true

<a class="footer-link" href="http://example.com/clustering-slides">bit.ly/clustering-slides</a>

---

# Overview

1. Introduction to Clustering
2. K-means Clustering
3. Hierarchical Clustering
4. Evaluation and Comparison of Clustering Methods
5. Application in Genomics

---

class: middle

# Introduction to Clustering

- Unsupervised learning technique
- Groups similar data points together
- No predefined labels or categories
- Used for:
  - Pattern discovery
  - Data compression
  - Preprocessing for other algorithms

---

class: middle

# K-means Clustering

- Partitioning method
- Aims to minimize within-cluster variation
- Algorithm:
  1. Choose K centroids
  2. Assign points to nearest centroid
  3. Recalculate centroids
  4. Repeat until convergence

---

class: middle

# K-means Clustering: Step-by-Step Visualization

.panelset[
.panel[.panel-name[Step 1: Choose Initial Centroids]

```{r kmeans-step1, echo=FALSE, fig.align='center'}
set.seed(838383)
pens <- penguins %>%
  select(bill_length_mm, bill_depth_mm) %>%
  drop_na()

init <- sample(seq_len(nrow(pens)), 3)

ggplot(pens) +
  geom_point(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(
    data = pens[init, ],
    aes(
      x = bill_length_mm, y = bill_depth_mm,
      color = c("Cluster 1", "Cluster 2", "Cluster 3")
    ),
    shape = "o", size = 12, stroke = 1
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```
]

.panel[.panel-name[Step 2: Assign Points]

```{r kmeans-step2, echo=FALSE, fig.align='center'}
closest_center <- Rfast::dista(as.matrix(pens), as.matrix(pens[init, ])) %>%
  apply(1, which.min)

ggplot(pens) +
  geom_point(
    aes(
      x = bill_length_mm, y = bill_depth_mm,
      color = as.factor(closest_center)
    ),
    size = 4
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```
]

.panel[.panel-name[Step 3: Recalculate Centroids]

```{r kmeans-step3, echo=FALSE, fig.align='center'}
centers <- pens %>%
  mutate(
    clust = closest_center
  ) %>%
  group_by(clust) %>%
  summarize_all(mean)

ggplot(pens) +
  geom_point(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(
    data = centers,
    aes(
      x = bill_length_mm, y = bill_depth_mm,
      color = c("Cluster 1", "Cluster 2", "Cluster 3")
    ),
    shape = "x", size = 12, stroke = 1
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```
]
]

---

class: middle

# K-means Clustering with tidyclust

.panelset[
.panel[.panel-name[Code]

```{r kmeans-tidyclust, eval=FALSE}
kmeans_spec <- k_means(num_clusters = 3) %>%
  set_engine("stats")

kmeans_fit <- kmeans_spec %>%
  fit(~ bill_length_mm + bill_depth_mm, data = penguins)

kmeans_clusters <- kmeans_fit %>%
  extract_cluster_assignment()

kmeans_centroids <- kmeans_fit %>%
  extract_centroids()
```
]

.panel[.panel-name[Visualization]

```{r kmeans-viz, eval=FALSE}
augment(kmeans_fit, new_data = penguins) %>%
  ggplot(aes(bill_length_mm, bill_depth_mm, color = .pred_cluster)) +
  geom_point() +
  labs(title = "K-means Clustering of Penguin Bill Dimensions")
```
]
]

---

class: middle

# Hierarchical Clustering

- Builds a tree-like structure (dendrogram)
- Two main types:
  - Agglomerative (bottom-up)
  - Divisive (top-down)
- Linkage methods:
  - Single
  - Complete
  - Average
  - Ward's

---

class: middle

# Hierarchical Clustering: Step-by-Step

.panelset[
.panel[.panel-name[Step 1: Each Point as a Cluster]

```{r hclust-step1, echo=FALSE, fig.align='center'}
fake_dat <- tibble(
  x = sort(runif(5)),
  y = runif(5),
  lab = letters[1:5]
)

fake_dat %>%
  ggplot(aes(x, y)) +
  geom_point(shape = fake_dat$lab, size = 4) +
  geom_point(shape = 1, size = 7, stroke = 1, color = "dark grey") +
  theme_minimal() +
  ylim(c(-0.1, 1.1)) +
  xlim(c(-0.1, 1.1))
```
]

.panel[.panel-name[Step 2: Merge Closest Clusters]

```{r hclust-step2, echo=FALSE, fig.align='center'}
fake_dat_2 <- bind_rows(
  fake_dat[-c(1:2), -3],
  summarize_all(fake_dat[1:2, -3], mean)
) %>%
  mutate(
    size = c(rep(1, 3), suppressWarnings(dist(fake_dat)[1]))
  )

fake_dat %>%
  ggplot(aes(x, y)) +
  geom_point(shape = fake_dat$lab, size = 4) +
  geom_point(
    data = fake_dat_2,
    aes(x = x, y = y),
    shape = 1,
    size = 7 / fake_dat_2$size,
    stroke = 1,
    color = "dark grey"
  ) +
  theme_minimal() +
  ylim(c(-0.1, 1.1)) +
  xlim(c(-0.1, 1.1))
```
]
]

---

class: middle

# Hierarchical Clustering with tidyclust

.panelset[
.panel[.panel-name[Code]

```{r hclust-tidyclust, eval=FALSE}
hclust_spec <- hier_clust(num_clusters = 3, linkage_method = "average") %>%
  set_engine("stats")

hclust_fit <- hclust_spec %>%
  fit(~ bill_length_mm + bill_depth_mm, data = penguins)

hclust_clusters <- hclust_fit %>%
  extract_cluster_assignment()

hclust_centroids <- hclust_fit %>%
  extract_centroids()
```
]

.panel[.panel-name[Visualization]

```{r hclust-viz, eval=FALSE}
hclust_fit %>%
  extract_fit_engine() %>%
  fviz_dend(k = 3, main = "Hierarchical Clustering Dendrogram")
```
]
]

---

class: middle

# Evaluation and Comparison

- Internal validation measures:
  - Silhouette score
  - Calinski-Harabasz index
  - Davies-Bouldin index
- External validation (if true labels are known):
  - Rand index
  - Adjusted Rand index
- Visual inspection and domain expertise

---

class: middle

# Silhouette Analysis

.panelset[
.panel[.panel-name[Code]

```{r silhouette, eval=FALSE}
kmeans_sil <- kmeans_fit %>% 
  silhouette_avg(penguins)

hclust_sil <- hclust_fit %>% 
  silhouette_avg(penguins)

tibble(
  Method = c("K-means", "Hierarchical"),
  Silhouette = c(kmeans_sil$.estimate, hclust_sil$.estimate)
) %>%
  knitr::kable()
```
]

.panel[.panel-name[Visualization]

```{r silhouette-viz, eval=FALSE}
plot_silhouette <- function(fit, data, title) {
  clusters <- fit %>% extract_cluster_assignment()
  sil <- cluster::silhouette(as.numeric(clusters$.cluster), dist(data))
  fviz_silhouette(sil) +
    labs(title = title)
}

plot_silhouette(kmeans_fit, penguins, "K-means Silhouette Plot")
```
]
]

---

class: middle

# Application in Genomics: NCI60 Dataset

- 6,830 gene expression measurements
- 64 cancer cell lines

.panelset[
.panel[.panel-name[Code]

```{r nci60-example, eval=FALSE}
data(NCI60, package = "ISLR")
nci60 <- as_tibble(NCI60$data, rownames = "gene") %>%
  mutate(across(-gene, scale)) %>%
  column_to_rownames("gene") %>%
  t() %>%
  as_tibble()  %>% 
  mutate(cancer_type = factor(NCI60$labs))

pca_result <- prcomp(nci60 %>% select(-cancer_type), scale. = TRUE)
```
]

.panel[.panel-name[Visualization]

```{r nci60-viz, eval=FALSE}
augment(pca_result, nci60) %>%
  ggplot(aes(.fittedPC1, .fittedPC2, color = cancer_type)) +
  geom_point() +
  labs(title = "PCA of NCI60 Gene Expression Data",
       x = "PC1", y = "PC2")
```
]
]

---

class: middle

# Conclusion

- Clustering is a powerful tool for unsupervised learning
- K-means and hierarchical clustering have different strengths
- Proper evaluation and interpretation are crucial
- Clustering has many applications in genomics and bioinformatics

---

class: middle

# References

1. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning.
2. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.
3. Jiang, D., Tang, C., & Zhang, A. (2004). Cluster Analysis for Gene Expression Data: A Survey.
4. Stuart, T., & Satija, R. (2019). Integrative single-cell analysis.

---

class: bottom, left, inverse

## Thank you!

### Find me at...
[GitHub: @renancesol](https://github.com/renancesol)  
[Twitter: @renancesol](https://twitter.com/renancesol)

[Page: http://example.com/renan](http://example.com/renan)