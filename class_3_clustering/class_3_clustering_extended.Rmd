---
title: "Class 3 - ML in Genomics: Clustering"
author: "Renan Escalante"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidymodels)
library(tidyclust)
library(palmerpenguins)
library(factoextra)
library(ggforce)
```

## Overview

1. Introduction to Clustering
2. K-means Clustering
3. Hierarchical Clustering
4. Evaluation and Comparison of Clustering Methods
5. Application in Genomics

## Introduction to Clustering

- Unsupervised learning technique
- Groups similar data points together
- No predefined labels or categories
- Used for:
  - Pattern discovery
  - Data compression
  - Preprocessing for other algorithms

## K-means Clustering

- Partitioning method
- Aims to minimize within-cluster variation
- Algorithm:
  1. Choose K centroids
  2. Assign points to nearest centroid
  3. Recalculate centroids
  4. Repeat until convergence

## K-means Clustering: Step-by-Step Visualization

```{r kmeans-steps, echo=FALSE}
set.seed(838383)
pens <- penguins %>%
  select(bill_length_mm, bill_depth_mm) %>%
  drop_na()

init <- sample(seq_len(nrow(pens)), 3)
```

## Step 1: Choose Initial Centroids

```{r kmeans-step1, echo=FALSE}
ggplot(pens) +
  geom_point(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(
    data = pens[init, ],
    aes(
      x = bill_length_mm, y = bill_depth_mm,
      color = c("Cluster 1", "Cluster 2", "Cluster 3")
    ),
    shape = "o", size = 12, stroke = 1
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## Step 2: Assign Points to Nearest Centroid

```{r kmeans-step2, echo=FALSE}
closest_center <- Rfast::dista(as.matrix(pens), as.matrix(pens[init, ])) %>%
  apply(1, which.min)

ggplot(pens) +
  geom_point(
    aes(
      x = bill_length_mm, y = bill_depth_mm,
      color = as.factor(closest_center)
    ),
    size = 4
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## Step 3: Recalculate Centroids

```{r kmeans-step3, echo=FALSE}
centers <- pens %>%
  mutate(
    clust = closest_center
  ) %>%
  group_by(clust) %>%
  summarize_all(mean)

ggplot(pens) +
  geom_point(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(
    data = centers,
    aes(
      x = bill_length_mm, y = bill_depth_mm,
      color = c("Cluster 1", "Cluster 2", "Cluster 3")
    ),
    shape = "x", size = 12, stroke = 1
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## Final K-means Result

```{r kmeans-final, echo=FALSE}
kmeans_result <- kmeans(pens, centers = pens[init, ])

ggplot(pens) +
  geom_point(
    aes(
      x = bill_length_mm,
      y = bill_depth_mm,
      color = as.factor(kmeans_result$cluster)
    )
  ) +
  geom_mark_ellipse(
    aes(
      x = bill_length_mm,
      y = bill_depth_mm,
      color = as.factor(kmeans_result$cluster)
    ),
    expand = unit(1, "mm")
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## K-means Clustering with tidyclust

```{r kmeans-tidyclust, eval=FALSE}
kmeans_spec <- k_means(num_clusters = 3) %>%
  set_engine("stats")

kmeans_fit <- kmeans_spec %>%
  fit(~ bill_length_mm + bill_depth_mm, data = penguins)

kmeans_clusters <- kmeans_fit %>%
  extract_cluster_assignment()

kmeans_centroids <- kmeans_fit %>%
  extract_centroids()
```

## Visualizing K-means Results

```{r kmeans-viz, eval=FALSE}
augment(kmeans_fit, new_data = penguins) %>%
  ggplot(aes(bill_length_mm, bill_depth_mm, color = .pred_cluster)) +
  geom_point() +
  labs(title = "K-means Clustering of Penguin Bill Dimensions")
```

## Hierarchical Clustering

- Builds a tree-like structure (dendrogram)
- Two main types:
  - Agglomerative (bottom-up)
  - Divisive (top-down)
- Linkage methods:
  - Single
  - Complete
  - Average
  - Ward's

## Hierarchical Clustering: Step-by-Step

```{r hclust-steps, echo=FALSE}
fake_dat <- tibble(
  x = sort(runif(5)),
  y = runif(5),
  lab = letters[1:5]
)
```

## Step 1: Each Point as a Cluster

```{r hclust-step1, echo=FALSE}
fake_dat %>%
  ggplot(aes(x, y)) +
  geom_point(shape = fake_dat$lab, size = 4) +
  geom_point(shape = 1, size = 7, stroke = 1, color = "dark grey") +
  theme_minimal() +
  ylim(c(-0.1, 1.1)) +
  xlim(c(-0.1, 1.1))
```

## Step 2: Merge Closest Clusters

```{r hclust-step2, echo=FALSE}
fake_dat_2 <- bind_rows(
  fake_dat[-c(1:2), -3],
  summarize_all(fake_dat[1:2, -3], mean)
) %>%
  mutate(
    size = c(rep(1, 3), suppressWarnings(dist(fake_dat)[1]))
  )

fake_dat %>%
  ggplot(aes(x, y)) +
  geom_point(shape = fake_dat$lab, size = 4) +
  geom_point(
    data = fake_dat_2,
    aes(x = x, y = y),
    shape = 1,
    size = 7 / fake_dat_2$size,
    stroke = 1,
    color = "dark grey"
  ) +
  theme_minimal() +
  ylim(c(-0.1, 1.1)) +
  xlim(c(-0.1, 1.1))
```

## Hierarchical Clustering with tidyclust

```{r hclust-tidyclust, eval=FALSE}
hclust_spec <- hier_clust(num_clusters = 3, linkage_method = "average") %>%
  set_engine("stats")

hclust_fit <- hclust_spec %>%
  fit(~ bill_length_mm + bill_depth_mm, data = penguins)

hclust_clusters <- hclust_fit %>%
  extract_cluster_assignment()

hclust_centroids <- hclust_fit %>%
  extract_centroids()
```

## Visualizing Hierarchical Clustering Results

```{r hclust-viz, eval=FALSE}
hclust_fit %>%
  extract_fit_engine() %>%
  fviz_dend(k = 3, main = "Hierarchical Clustering Dendrogram")
```

## Evaluation and Comparison

- Internal validation measures:
  - Silhouette score
  - Calinski-Harabasz index
  - Davies-Bouldin index
- External validation (if true labels are known):
  - Rand index
  - Adjusted Rand index
- Visual inspection and domain expertise

## Silhouette Analysis

```{r silhouette, eval=FALSE}
kmeans_sil <- kmeans_fit %>% 
  silhouette_avg(penguins)

hclust_sil <- hclust_fit %>% 
  silhouette_avg(penguins)

tibble(
  Method = c("K-means", "Hierarchical"),
  Silhouette = c(kmeans_sil$.estimate, hclust_sil$.estimate)
) %>%
  knitr::kable()
```

## Determining Optimal Number of Clusters

```{r elbow-method, eval=FALSE}
tibble(k = 1:10) %>%
  mutate(
    kmeans = map(k, ~k_means(num_clusters = .x) %>% 
                   fit(~., data = penguins) %>% 
                   sse_within_total()),
    kmeans_sse = map_dbl(kmeans, ~.x$.estimate)
  ) %>%
  ggplot(aes(k, kmeans_sse)) +
  geom_line() +
  geom_point() +
  labs(title = "Elbow Method for K-means",
       x = "Number of Clusters (k)",
       y = "Within-cluster Sum of Squares")
```

## Application in Genomics: NCI60 Dataset

- 6,830 gene expression measurements
- 64 cancer cell lines

```{r nci60-example, eval=FALSE}
data(NCI60, package = "ISLR")
nci60 <- as_tibble(NCI60$data, rownames = "gene") %>%
  mutate(across(-gene, scale)) %>%
  column_to_rownames("gene") %>%
  t() %>%
  as_tibble()  %>% 
  mutate(cancer_type = factor(NCI60$labs))

pca_result <- prcomp(nci60 %>% select(-cancer_type), scale. = TRUE)

augment(pca_result, nci60) %>%
  ggplot(aes(.fittedPC1, .fittedPC2, color = cancer_type)) +
  geom_point() +
  labs(title = "PCA of NCI60 Gene Expression Data",
       x = "PC1", y = "PC2")
```

## Conclusion

- Clustering is a powerful tool for unsupervised learning
- K-means and hierarchical clustering have different strengths
- Proper evaluation and interpretation are crucial
- Clustering has many applications in genomics and bioinformatics

## References

1. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning.
2. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.
3. Jiang, D., Tang, C., & Zhang, A. (2004). Cluster Analysis for Gene Expression Data: A Survey.
4. Stuart, T., & Satija, R. (2019). Integrative single-cell analysis.