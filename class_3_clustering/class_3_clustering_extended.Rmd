---
title: "Class 3 - ML in Genomics: Clustering"
author: "Renan Escalante"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidymodels)
library(tidyclust)
library(palmerpenguins)
library(factoextra)
library(ggforce)
```

## Overview

1. Introduction to Clustering
2. K-means Clustering
3. Hierarchical Clustering
4. Evaluation and Comparison of Clustering Methods
5. Application in Genomics

## Introduction to Clustering

- Unsupervised learning technique
- Groups similar data points together
- No predefined labels or categories
- Used for:
  - Pattern discovery
  - Data compression
  - Preprocessing for other algorithms

## K-means Clustering

- Partitioning method
- Aims to minimize within-cluster variation
- Algorithm:
  1. Choose K centroids
  2. Assign points to nearest centroid
  3. Recalculate centroids
  4. Repeat until convergence

## K-means Clustering: Step-by-Step Visualization

```{r kmeans-steps, echo=FALSE}
set.seed(838383)
pens <- penguins %>%
  select(bill_length_mm, bill_depth_mm) %>%
  drop_na()

init <- sample(seq_len(nrow(pens)), 3)
```

## Step 1: Choose Initial Centroids

```{r kmeans-step1, echo=FALSE}
ggplot(pens) +
  geom_point(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(
    data = pens[init, ],
    aes(
      x = bill_length_mm, y = bill_depth_mm,
      color = c("Cluster 1", "Cluster 2", "Cluster 3")
    ),
    shape = "o", size = 12, stroke = 1
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## Step 2: Assign Points to Nearest Centroid

```{r kmeans-step2, echo=FALSE}
closest_center <- Rfast::dista(as.matrix(pens), as.matrix(pens[init, ])) %>%
  apply(1, which.min)

ggplot(pens) +
  geom_point(
    aes(
      x = bill_length_mm, y = bill_depth_mm,
      color = as.factor(closest_center)
    ),
    size = 4
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## Step 3: Recalculate Centroids

```{r kmeans-step3, echo=FALSE}
centers <- pens %>%
  mutate(
    clust = closest_center
  ) %>%
  group_by(clust) %>%
  summarize_all(mean)

ggplot(pens) +
  geom_point(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(
    data = centers,
    aes(
      x = bill_length_mm, y = bill_depth_mm,
      color = c("Cluster 1", "Cluster 2", "Cluster 3")
    ),
    shape = "x", size = 12, stroke = 1
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## Final K-means Result

```{r kmeans-final, echo=FALSE}
kmeans_result <- kmeans(pens, centers = pens[init, ])

ggplot(pens) +
  geom_point(
    aes(
      x = bill_length_mm,
      y = bill_depth_mm,
      color = as.factor(kmeans_result$cluster)
    )
  ) +
  geom_mark_ellipse(
    aes(
      x = bill_length_mm,
      y = bill_depth_mm,
      color = as.factor(kmeans_result$cluster)
    ),
    expand = unit(1, "mm")
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## K-means Clustering with tidyclust

```{r kmeans-tidyclust, eval=FALSE}
kmeans_spec <- k_means(num_clusters = 3) %>%
  set_engine("stats")

kmeans_fit <- kmeans_spec %>%
  fit(~ bill_length_mm + bill_depth_mm, data = penguins)

kmeans_clusters <- kmeans_fit %>%
  extract_cluster_assignment()

kmeans_centroids <- kmeans_fit %>%
  extract_centroids()
```

## Visualizing K-means Results

```{r kmeans-viz, eval=FALSE}
augment(kmeans_fit, new_data = penguins) %>%
  ggplot(aes(bill_length_mm, bill_depth_mm, color = .pred_cluster)) +
  geom_point() +
  labs(title = "K-means Clustering of Penguin Bill Dimensions")
```

## Hierarchical Clustering

- Builds a tree-like structure (dendrogram)
- Two main types:
  - Agglomerative (bottom-up)
  - Divisive (top-down)
- Linkage methods:
  - Single
  - Complete
  - Average
  - Ward's

## Hierarchical Clustering: Step-by-Step

```{r hclust-steps, echo=FALSE}
fake_dat <- tibble(
  x = sort(runif(5)),
  y = runif(5),
  lab = letters[1:5]
)
```

## Step 1: Each Point as a Cluster

```{r hclust-step1, echo=FALSE}
fake_dat %>%
  ggplot(aes(x, y)) +
  geom_point(shape = fake_dat$lab, size = 4) +
  geom_point(shape = 1, size = 7, stroke = 1, color = "dark grey") +
  theme_minimal() +
  ylim(c(-0.1, 1.1)) +
  xlim(c(-0.1, 1.1))
```

## Step 2: Merge Closest Clusters

```{r hclust-step2, echo=FALSE}
fake_dat_2 <- bind_rows(
  fake_dat[-c(1:2), -3],
  summarize_all(fake_dat[1:2, -3], mean)
) %>%
  mutate(
    size = c(rep(1, 3), suppressWarnings(dist(fake_dat)[1]))
  )

fake_dat %>%
  ggplot(aes(x, y)) +
  geom_point(shape = fake_dat$lab, size = 4) +
  geom_point(
    data = fake_dat_2,
    aes(x = x, y = y),
    shape = 1,
    size = 7 / fake_dat_2$size,
    stroke = 1,
    color = "dark grey"
  ) +
  theme_minimal() +
  ylim(c(-0.1, 1.1)) +
  xlim(c(-0.1, 1.1))
```

## Hierarchical Clustering with tidyclust

```{r hclust-tidyclust, eval=FALSE}
hclust_spec <- hier_clust(num_clusters = 3, linkage_method = "average") %>%
  set_engine("stats")

hclust_fit <- hclust_spec %>%
  fit(~ bill_length_mm + bill_depth_mm, data = penguins)

hclust_clusters <- hclust_fit %>%
  extract_cluster_assignment()

hclust_centroids <- hclust_fit %>%
  extract_centroids()
```

## Visualizing Hierarchical Clustering Results

```{r hclust-viz, eval=FALSE}
hclust_fit %>%
  extract_fit_engine() %>%
  fviz_dend(k = 3, main = "Hierarchical Clustering Dendrogram")
```

## Evaluation and Comparison

- Internal validation measures:
  - Silhouette score
  - Calinski-Harabasz index
  - Davies-Bouldin index
- External validation (if true labels are known):
  - Rand index
  - Adjusted Rand index
- Visual inspection and domain expertise

## Silhouette Analysis

```{r silhouette, eval=FALSE}
kmeans_sil <- kmeans_fit %>% 
  silhouette_avg(penguins)

hclust_sil <- hclust_fit %>% 
  silhouette_avg(penguins)

tibble(
  Method = c("K-means", "Hierarchical"),
  Silhouette = c(kmeans_sil$.estimate, hclust_sil$.estimate)
) %>%
  knitr::kable()
```

## Determining Optimal Number of Clusters

```{r elbow-method, eval=FALSE}
tibble(k = 1:10) %>%
  mutate(
    kmeans = map(k, ~k_means(num_clusters = .x) %>% 
                   fit(~., data = penguins) %>% 
                   sse_within_total()),
    kmeans_sse = map_dbl(kmeans, ~.x$.estimate)
  ) %>%
  ggplot(aes(k, kmeans_sse)) +
  geom_line() +
  geom_point() +
  labs(title = "Elbow Method for K-means",
       x = "Number of Clusters (k)",
       y = "Within-cluster Sum of Squares")
```

## Application in Genomics: NCI60 Dataset

- 6,830 gene expression measurements
- 64 cancer cell lines

```{r nci60-example, eval=FALSE}
data(NCI60, package = "ISLR")
nci60 <- as_tibble(NCI60$data, rownames = "gene") %>%
  mutate(across(-gene, scale)) %>%
  column_to_rownames("gene") %>%
  t() %>%
  as_tibble()  %>% 
  mutate(cancer_type = factor(NCI60$labs))

pca_result <- prcomp(nci60 %>% select(-cancer_type), scale. = TRUE)

augment(pca_result, nci60) %>%
  ggplot(aes(.fittedPC1, .fittedPC2, color = cancer_type)) +
  geom_point() +
  labs(title = "PCA of NCI60 Gene Expression Data",
       x = "PC1", y = "PC2")
```

## Tuning Cluster Models

- In unsupervised settings, there's no objective measure of success
- We can still vary inputs and quantify results
- Goal: Select appropriate number of clusters based on metrics

## Cross-Validation for Clustering

```{r cv-setup, eval=FALSE}
penguins_cv <- vfold_cv(penguins, v = 5)

kmeans_spec <- k_means(num_clusters = tune()) %>%
  set_engine("stats")

penguins_rec <- recipe(~ bill_length_mm + bill_depth_mm, data = penguins)

kmeans_wflow <- workflow(penguins_rec, kmeans_spec)

clust_num_grid <- grid_regular(num_clusters(), levels = 10)
```

## Tuning with tune_cluster()

```{r tune-cluster, eval=FALSE}
res <- tune_cluster(
  kmeans_wflow,
  resamples = penguins_cv,
  grid = clust_num_grid,
  metrics = cluster_metric_set(sse_within_total, sse_total, sse_ratio)
)

res_metrics <- res %>% collect_metrics()
```

## Visualizing Tuning Results

```{r tune-viz, eval=FALSE}
res_metrics %>%
  filter(.metric == "sse_ratio") %>%
  ggplot(aes(x = num_clusters, y = mean)) +
  geom_point() +
  geom_line() +
  theme_minimal() +
  ylab("mean WSS/TSS ratio, over 5 folds") +
  xlab("Number of clusters") +
  scale_x_continuous(breaks = 1:10)
```

## Interpreting the Elbow Plot

- Look for an "elbow" or notable bend in the plot
- WSS/TSS ratio decreases with more clusters
- Choose number of clusters where decrease starts to level off
- For penguins, 3 or 4 clusters might be appropriate

## Silhouette Analysis

```{r silhouette-analysis, eval=FALSE}
kmeans_silhouette <- kmeans_fit %>% 
  silhouette_avg(penguins_scaled)

plot_silhouette <- function(fit, data, title) {
  clusters <- fit %>% extract_cluster_assignment()
  sil <- cluster::silhouette(as.numeric(clusters$.cluster), dist(data))
  fviz_silhouette(sil) +
    labs(title = title)
}

plot_silhouette(kmeans_fit, penguins_scaled, "K-means Silhouette Plot")
```

## Metrics for Clustering

- Sum of Squared Errors (SSE)
  - Within-cluster SSE
  - Total SSE
  - SSE ratio (WSS/TSS)
- Silhouette score
- Can be used with different distance measures

```{r cluster-metrics, eval=FALSE}
kmeans_fit %>% sse_within_total()
kmeans_fit %>% sse_total()
kmeans_fit %>% sse_ratio()
kmeans_fit %>% silhouette_avg(penguins_scaled)
```

## Custom Distance Measures

```{r custom-distance, eval=FALSE}
my_dist <- function(x, y) {
  Rfast::dista(x, y, method = "manhattan")
}

kmeans_fit %>% sse_ratio(dist_fun = my_dist)
```

## Conclusion on Tuning and Metrics

- Tuning helps find optimal number of clusters
- Multiple metrics provide different perspectives on cluster quality
- Visualization aids in interpreting results
- Consider domain knowledge when selecting final model


## Conclusion

- Clustering is a powerful tool for unsupervised learning
- K-means and hierarchical clustering have different strengths
- Proper evaluation and interpretation are crucial
- Clustering has many applications in genomics and bioinformatics

## References

1. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning.
2. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.
3. Jiang, D., Tang, C., & Zhang, A. (2004). Cluster Analysis for Gene Expression Data: A Survey.
4. Stuart, T., & Satija, R. (2019). Integrative single-cell analysis.