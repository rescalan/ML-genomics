---
title: "Class 4 - ML in genomics: Regularized Linear Regression and Evaluation Metrics"
author: "Renan Escalante"
date: "`r Sys.Date()`"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(tidymodels)
```

## Regularized Linear Regression and Penalty

In tidymodels, we can tune the penalty using:

```{r, echo=TRUE, eval=FALSE}
tune_spec <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")
```

## Intuition Behind Penalty

- Think of penalty as a "budget constraint" on model coefficients
- Higher penalty: shrinks coefficients toward zero, simpler model
- Lower penalty: allows larger coefficients, potentially overfitting
- Helps control model complexity and prevent overfitting

## Types of Penalties

The `mixture` parameter determines the type of penalty:

- `mixture = 0`: Ridge regression (L2 penalty)
- `mixture = 1`: Lasso regression (L1 penalty)
- `0 < mixture < 1`: Elastic net (combination of L1 and L2)

## L2 Penalty (Ridge Regression)

- Shrinks all coefficients toward zero, but rarely sets them exactly to zero
- Good for handling multicollinearity
- Keeps all features in the model

## L1 Penalty (Lasso Regression)

- Can shrink coefficients exactly to zero, performing feature selection
- Good when you suspect many features are irrelevant
- Produces sparse models

## When to Use Each Type

- Use Ridge when you want to keep all features and handle multicollinearity
- Use Lasso when you want automatic feature selection
- Use Elastic Net as a compromise between Ridge and Lasso

## Tuning in tidymodels

- tidymodels allows tuning both `penalty` and `mixture`
- Explores a range of regularization approaches
- Helps find the best balance between model complexity and performance
- Optimizes for your specific dataset

## Evaluation Metrics for Regression Models

We'll discuss three common metrics for evaluating regression models:

1. RMSE (Root Mean Square Error)
2. R-squared (R²)
3. MAE (Mean Absolute Error)

## RMSE (Root Mean Square Error)

- Measures the standard deviation of the residuals (prediction errors)
- Lower values indicate better fit
- Same unit as the dependent variable
- Penalizes larger errors more heavily

## RMSE: Equation

The formula for RMSE is:

$$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}$$

Where:
- $n$ is the number of observations
- $y_i$ is the actual value
- $\hat{y}_i$ is the predicted value

## RMSE: Intuition

Imagine you're predicting house prices:
- An RMSE of $50,000 means that, on average, your predictions are off by about $50,000.
- However, this could mean many small errors or a few very large ones.
- RMSE is like measuring the size of a "typical" error, but giving extra weight to the big mistakes.

## R-squared (R²)

- Represents the proportion of variance in the dependent variable explained by the independent variables
- Ranges from 0 to 1 (or 0% to 100%)
- Higher values indicate better fit

## R-squared: Equation

The formula for R-squared is:

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$

Where:
- $SS_{res} = \sum_{i=1}^n (y_i - \hat{y}_i)^2$ (Sum of squares of residuals)
- $SS_{tot} = \sum_{i=1}^n (y_i - \bar{y})^2$ (Total sum of squares)
- $\bar{y}$ is the mean of observed data

## R-squared: Intuition

Think of R² as a percentage of "credit" your model gets:
- If R² = 0.7, your model "explains" 70% of the variability in the data.
- It's like saying, "My model captures 70% of the patterns in the data."
- The remaining 30% is either random noise or patterns your model missed.

## MAE (Mean Absolute Error)

- Measures the average magnitude of errors in a set of predictions
- Lower values indicate better fit
- Same unit as the dependent variable
- Less sensitive to outliers compared to RMSE

## MAE: Equation

The formula for MAE is:

$$MAE = \frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i|$$

Where:
- $n$ is the number of observations
- $y_i$ is the actual value
- $\hat{y}_i$ is the predicted value

## MAE: Intuition

Back to our house price prediction:
- An MAE of $40,000 means that, on average, your predictions are off by $40,000.
- Unlike RMSE, this treats all errors equally. A $60,000 error is treated as 1.5 times worse than a $40,000 error, not 2.25 times worse.
- MAE is like saying, "On average, I miss the true price by $40,000, sometimes more, sometimes less."

## Comparing Metrics in tidymodels

In tidymodels, you can easily calculate these metrics:

```{r, echo=TRUE, eval=FALSE}
model_metrics <- metric_set(rmse, rsq, mae)

results <- fit_resamples(
  model_spec,
  recipe,
  resamples = cv_folds,
  metrics = model_metrics
)
```

This allows you to evaluate your models using multiple metrics simultaneously.

## Regression Metrics Overview (1/4)

| Metric | Function | Definition | When to Use |
|--------|----------|------------|-------------|
| Root Mean Squared Error | rmse(), rmse_vec() | Square root of the average of squared differences between prediction and actual observation | When you want to penalize large errors more heavily; useful for applications where large errors are particularly undesirable |
| R-squared | rsq(), rsq_vec() | Proportion of variance in the dependent variable that is predictable from the independent variable(s) | When you want a metric that's easy to interpret across different datasets; to explain model performance to non-technical stakeholders |
| Traditional R-squared | rsq_trad(), rsq_trad_vec() | Similar to R-squared, but calculated using a traditional formula | When comparing to models fit outside of tidymodels that use the traditional R-squared calculation |

## Regression Metrics Overview (2/4)

| Metric | Function | Definition | When to Use |
|--------|----------|------------|-------------|
| Mean Signed Deviation | msd(), msd_vec() | Average of the differences between prediction and actual observation | When the direction of error (over or under prediction) is important |
| Mean Absolute Error | mae(), mae_vec() | Average of the absolute differences between prediction and actual observation | When you want to treat all errors equally, regardless of their direction; less sensitive to outliers than RMSE |
| Mean Percentage Error | mpe(), mpe_vec() | Average percentage difference between predicted and actual values | When you want to measure bias in your predictions (tendency to over or underestimate) |
| Mean Absolute Percent Error | mape(), mape_vec() | Average of the absolute percentage differences between predicted and actual values | When you want to express error in percentage terms, making it easier to compare across different scales |

## Regression Metrics Overview (3/4)

| Metric | Function | Definition | When to Use |
|--------|----------|------------|-------------|
| Symmetric Mean Absolute Percentage Error | smape(), smape_vec() | Variation of MAPE that is more robust to outliers and handles zero values better | When dealing with data that includes zeros or when you want a percentage error that treats over and under predictions more symmetrically |
| Mean Absolute Scaled Error | mase(), mase_vec() | MAE scaled by the MAE of a naive forecast | When you want to compare forecast accuracy across different scales |
| Concordance Correlation Coefficient | ccc(), ccc_vec() | Measures both precision and accuracy to determine how far the observed data deviate from the line of perfect concordance | When you want to assess both how close the relationship is to the line y=x and how far the points deviate from this line |
| Ratio of Performance to Inter-Quartile | rpiq(), rpiq_vec() | Ratio of the inter-quartile range of the observed values to the RMSE | When dealing with non-normally distributed data; often used in spectroscopy and chemometrics |

## Regression Metrics Overview (4/4)

| Metric | Function | Definition | When to Use |
|--------|----------|------------|-------------|
| Ratio of Performance to Deviation | rpd(), rpd_vec() | Ratio of the standard deviation of the observed values to the RMSE | When you want to assess model performance relative to the overall variability in the data |
| Huber Loss | huber_loss(), huber_loss_vec() | Loss function that is less sensitive to outliers in data than the squared error loss | When you want a compromise between squared error loss and absolute error loss; robust to outliers |
| Pseudo-Huber Loss | huber_loss_pseudo(), huber_loss_pseudo_vec() | Smooth approximation of the Huber loss | When you want the benefits of Huber loss but need a continuously differentiable function |
| Index of Ideality of Correlation | iic(), iic_vec() | Metric that combines correlation coefficient with the slope of the regression line | When you want to assess both the linearity and the accuracy of predictions |
| Poisson Log Loss | poisson_log_loss(), poisson_log_loss_vec() | Logarithmic loss function specific to Poisson-distributed data | When working with count data or rates, especially in fields like epidemiology or reliability analysis |