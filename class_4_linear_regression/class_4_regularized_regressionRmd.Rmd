---
title: "Class 4 - ML in genomics: Regularized Linear Regression and Evaluation Metrics"
author: "Renan Escalante"
date: "`r Sys.Date()`"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(tidymodels)
```

## Regularized Linear Regression and Penalty

In tidymodels, we can tune the penalty using:

```{r, echo=TRUE, eval=FALSE}
tune_spec <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")
```

## Intuition Behind Penalty

- Think of penalty as a "budget constraint" on model coefficients
- Higher penalty: shrinks coefficients toward zero, simpler model
- Lower penalty: allows larger coefficients, potentially overfitting
- Helps control model complexity and prevent overfitting

## Types of Penalties

The `mixture` parameter determines the type of penalty:

- `mixture = 0`: Ridge regression (L2 penalty)
- `mixture = 1`: Lasso regression (L1 penalty)
- `0 < mixture < 1`: Elastic net (combination of L1 and L2)

## L2 Penalty (Ridge Regression)

- Shrinks all coefficients toward zero, but rarely sets them exactly to zero
- Good for handling multicollinearity
- Keeps all features in the model

## L1 Penalty (Lasso Regression)

- Can shrink coefficients exactly to zero, performing feature selection
- Good when you suspect many features are irrelevant
- Produces sparse models

## When to Use Each Type

- Use Ridge when you want to keep all features and handle multicollinearity
- Use Lasso when you want automatic feature selection
- Use Elastic Net as a compromise between Ridge and Lasso

## Tuning in tidymodels

- tidymodels allows tuning both `penalty` and `mixture`
- Explores a range of regularization approaches
- Helps find the best balance between model complexity and performance
- Optimizes for your specific dataset

## Evaluation Metrics for Regression Models

We'll discuss three common metrics for evaluating regression models:

1. RMSE (Root Mean Square Error)
2. R-squared (R²)
3. MAE (Mean Absolute Error)

## RMSE (Root Mean Square Error)

- Measures the standard deviation of the residuals (prediction errors)
- Lower values indicate better fit
- Same unit as the dependent variable
- Penalizes larger errors more heavily

## RMSE: Equation

The formula for RMSE is:

$$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}$$

Where:
- $n$ is the number of observations
- $y_i$ is the actual value
- $\hat{y}_i$ is the predicted value

## RMSE: Intuition

Imagine you're predicting house prices:
- An RMSE of $50,000 means that, on average, your predictions are off by about $50,000.
- However, this could mean many small errors or a few very large ones.
- RMSE is like measuring the size of a "typical" error, but giving extra weight to the big mistakes.

## R-squared (R²)

- Represents the proportion of variance in the dependent variable explained by the independent variables
- Ranges from 0 to 1 (or 0% to 100%)
- Higher values indicate better fit

## R-squared: Equation

The formula for R-squared is:

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$

Where:
- $SS_{res} = \sum_{i=1}^n (y_i - \hat{y}_i)^2$ (Sum of squares of residuals)
- $SS_{tot} = \sum_{i=1}^n (y_i - \bar{y})^2$ (Total sum of squares)
- $\bar{y}$ is the mean of observed data

## R-squared: Intuition

Think of R² as a percentage of "credit" your model gets:
- If R² = 0.7, your model "explains" 70% of the variability in the data.
- It's like saying, "My model captures 70% of the patterns in the data."
- The remaining 30% is either random noise or patterns your model missed.

## MAE (Mean Absolute Error)

- Measures the average magnitude of errors in a set of predictions
- Lower values indicate better fit
- Same unit as the dependent variable
- Less sensitive to outliers compared to RMSE

## MAE: Equation

The formula for MAE is:

$$MAE = \frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i|$$

Where:
- $n$ is the number of observations
- $y_i$ is the actual value
- $\hat{y}_i$ is the predicted value

## MAE: Intuition

Back to our house price prediction:
- An MAE of $40,000 means that, on average, your predictions are off by $40,000.
- Unlike RMSE, this treats all errors equally. A $60,000 error is treated as 1.5 times worse than a $40,000 error, not 2.25 times worse.
- MAE is like saying, "On average, I miss the true price by $40,000, sometimes more, sometimes less."

## When to Use Each Metric

RMSE:
- When you want to heavily penalize large errors
- When your target variable is in a meaningful scale
- When outliers are particularly undesirable

R-squared:
- When you want a metric that's easy to interpret across different datasets
- To compare models for the same dependent variable
- When you want to explain the model's performance to non-technical stakeholders

MAE:
- When you want to treat all errors equally (not penalize large errors as heavily as RMSE)
- When working with datasets that may contain outliers
- When you need a metric that's easily interpretable in terms of the original units

## Comparing Metrics in tidymodels

In tidymodels, you can easily calculate these metrics:

```{r, echo=TRUE, eval=FALSE}
model_metrics <- metric_set(rmse, rsq, mae)

results <- fit_resamples(
  model_spec,
  recipe,
  resamples = cv_folds,
  metrics = model_metrics
)
```

This allows you to evaluate your models using multiple metrics simultaneously.