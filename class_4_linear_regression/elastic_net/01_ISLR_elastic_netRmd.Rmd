---
title: "01 ISLR elastic net"
output: html_document
date: "2024-09-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(tidyverse)
```

# Linear Model Selection and Regularization

This document demonstrates linear model selection and regularization techniques using the tidymodels framework in R. We'll be working with the Hitters dataset from the ISLR package.

First, let's load the necessary libraries and prepare our data:

```r
library(tidymodels)
library(ISLR)

Hitters <- as_tibble(Hitters) %>%
  filter(!is.na(Salary))
```

We load the tidymodels and ISLR libraries. Then we convert the Hitters dataset to a tibble and remove any rows with missing Salary values.

## Ridge Regression

Let's start with ridge regression. We'll create a model specification for ridge regression:

```r
ridge_spec <- linear_reg(mixture = 0, penalty = 0) %>%
  set_mode("regression") %>%
  set_engine("glmnet")
```

Here, we set `mixture = 0` for ridge regression and initially set `penalty = 0`. We'll tune this later.

Now, let's fit the model:

```r
ridge_fit <- fit(ridge_spec, Salary ~ ., data = Hitters)
```

This fits the ridge regression model using all variables to predict Salary.

We can examine the model coefficients:

```r
tidy(ridge_fit)
```

The `tidy()` function gives us a tidy tibble of the model coefficients.

Let's see how the coefficients change with different penalty values:

```r
tidy(ridge_fit, penalty = 11498)
tidy(ridge_fit, penalty = 705)
tidy(ridge_fit, penalty = 50)
```

As the penalty increases, the coefficients shrink towards zero.

We can visualize how coefficients change with penalty:

```r
ridge_fit %>%
  autoplot()
```

This plot shows how each coefficient changes as the penalty (lambda) increases.

## Tuning Ridge Regression

To find the optimal penalty, we'll use cross-validation. First, let's split our data:

```r
Hitters_split <- initial_split(Hitters, strata = "Salary")
Hitters_train <- training(Hitters_split)
Hitters_test <- testing(Hitters_split)
Hitters_fold <- vfold_cv(Hitters_train, v = 10)
```

We create training and testing sets, and set up 10-fold cross-validation.

Now, let's create a recipe for preprocessing:

```r
ridge_recipe <-
  recipe(formula = Salary ~ ., data = Hitters_train) %>%
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())
```

This recipe handles nominal predictors, removes zero-variance predictors, and normalizes all predictors.

Let's update our model specification to tune the penalty:

```r
ridge_spec <-
  linear_reg(penalty = tune(), mixture = 0) %>%
  set_mode("regression") %>%
  set_engine("glmnet")
```

Now we create a workflow combining the recipe and model:

```r
ridge_workflow <- workflow() %>%
  add_recipe(ridge_recipe) %>%
  add_model(ridge_spec)
```

We'll create a grid of penalty values to try:

```r
penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)
```

Now we can tune the model:

```r
tune_res <- tune_grid(
  ridge_workflow,
  resamples = Hitters_fold,
  grid = penalty_grid
)
```

Let's visualize the results:

```r
autoplot(tune_res)
```

This plot shows how different metrics change with the penalty value.

We can select the best penalty value:

```r
best_penalty <- select_best(tune_res, metric = "rsq")
```

Now let's finalize our workflow with the best penalty and fit it to the training data:

```r
ridge_final <- finalize_workflow(ridge_workflow, best_penalty)
ridge_final_fit <- fit(ridge_final, data = Hitters_train)
```

Finally, we can evaluate the model on the test set:

```r
augment(ridge_final_fit, new_data = Hitters_test) %>%
  rsq(truth = Salary, estimate = .pred)
```

This gives us the R-squared value for our final ridge regression model on the test set.

## Lasso Regression

The process for Lasso regression is very similar to Ridge regression. The main difference is setting `mixture = 1` in the model specification:

```r
lasso_spec <-
  linear_reg(penalty = tune(), mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet")
```

We can then follow the same workflow as we did for ridge regression, including creating a recipe, workflow, tuning the model, and evaluating the results.

## Principal Components Regression

For PCR, we use a linear regression model but include PCA in our preprocessing recipe:

```r
pca_recipe <-
  recipe(formula = Salary ~ ., data = Hitters_train) %>%
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), threshold = tune())
```

We tune the `threshold` parameter in `step_pca()` to determine how many principal components to retain.

## Partial Least Squares

PLS is similar to PCR, but we use `step_pls()` instead of `step_pca()`:

```r
pls_recipe <-
  recipe(formula = Salary ~ ., data = Hitters_train) %>%
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_pls(all_predictors(), num_comp = tune(), outcome = "Salary")
```

We tune the `num_comp` parameter to determine how many PLS components to use.

The rest of the process (creating a workflow, tuning, and evaluating) follows the same pattern as the previous methods.

## Conclusion

This document has demonstrated how to implement and tune various linear model selection and regularization techniques using the tidymodels framework in R. These techniques include Ridge Regression, Lasso Regression, Principal Components Regression, and Partial Least Squares. Each method offers different approaches to handling multicollinearity and feature selection in linear models.