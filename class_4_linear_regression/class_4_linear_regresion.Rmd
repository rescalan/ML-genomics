---
title: "Class 4 - ML in genomics: Linear Regression"
author: "Renan Escalante"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 3)
library(tidyverse)
library(broom)
library(ggplot2)
library(performance)
```

## Introduction to Linear Regression

- Fundamental statistical technique for modeling relationships
- Predicts a continuous outcome variable based on one or more predictor variables
- Assumes a linear relationship between predictors and outcome

## Simple Linear Regression

- One predictor variable (X) and one outcome variable (Y)
- Equation: Y = β0 + β1X + ε
  - β0: Y-intercept
  - β1: Slope
  - ε: Error term

```{r simple-linear-regression, echo=FALSE, fig.width=6, fig.height=3.5}
set.seed(123)
x <- seq(1, 10, length.out = 100)
y <- 2 + 3*x + rnorm(100, sd = 2)
data <- tibble(x = x, y = y)

ggplot(data, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Simple Linear Regression Example")
```

## Multiple Linear Regression

- Multiple predictor variables (X1, X2, ..., Xn)
- Equation: Y = β0 + β1X1 + β2X2 + ... + βnXn + ε

## Fitting a Linear Model in R

```{r fit-linear-model}
# Using mtcars dataset
mod <- lm(mpg ~ wt + qsec, data = mtcars)

# View model summary
tidy(mod)
glance(mod)
```

## Interpreting Coefficients

- Coefficients represent the change in Y for a one-unit change in X
- Intercept (β0): Expected value of Y when all predictors are zero
- For each predictor (βi): Change in Y for a one-unit increase in Xi, holding other predictors constant

## Coefficient Plot

Visualizing coefficients and their confidence intervals:

```{r coefficient-plot, fig.width=6, fig.height=3.5}
d <- tidy(mod, conf.int = TRUE)

ggplot(d, aes(estimate, term, xmin = conf.low, xmax = conf.high, height = 0)) +
  geom_point() +
  geom_vline(xintercept = 0, lty = 4) +
  geom_errorbarh() +
  labs(title = "Coefficient Plot", x = "Estimate", y = "Term")
```

## Model Evaluation: R-squared

- R-squared measures the proportion of variance explained by the model
- Ranges from 0 to 1 (0% to 100%)
- Higher values indicate better fit, but beware of overfitting

```{r r-squared}
glance(mod) %>% select(r.squared, adj.r.squared)
```

## Model Augmentation

Adding fitted values and residuals to the original data:

```{r augment-model}
augment(mod)
```

## Predictions with New Data

```{r predict-new-data}
newdata <- mtcars %>%
  head(6) %>%
  mutate(wt = wt + 1)

augment(mod, newdata = newdata)
```


## Residual Analysis

- Residuals: Differences between observed and predicted values
- Assumptions:
  1. Linearity
  2. Independence
  3. Homoscedasticity
  4. Normality

```{r residual-plot, echo=FALSE, fig.width=6, fig.height=3.5}
au <- augment(mod)
ggplot(au, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residual Plot", x = "Fitted Values", y = "Residuals")
```

## Diagnostic Plots: Leverage vs Standardized Residuals

```{r leverage-residuals-plot, fig.width=6, fig.height=3.5}
ggplot(au, aes(.hat, .std.resid)) +
  geom_vline(size = 2, colour = "white", xintercept = 0) +
  geom_hline(size = 2, colour = "white", yintercept = 0) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(title = "Leverage vs Standardized Residuals",
       x = "Leverage", y = "Standardized Residuals")
```

## Diagnostic Plots: Cook's Distance

```{r cooks-distance-plot, fig.width=6, fig.height=3.5}
ggplot(au, aes(.hat, .cooksd)) +
  geom_vline(xintercept = 0, colour = NA) +
  geom_abline(slope = seq(0, 3, by = 0.5), colour = "white") +
  geom_smooth(se = FALSE) +
  geom_point() +
  labs(title = "Cook's Distance Plot",
       x = "Leverage", y = "Cook's Distance")
```

## Model Diagnostics with {performance}

The {performance} package provides tools for model evaluation:

```{r performance-checks, fig.width=8, fig.height=4}
check_model(mod)
```

## Comparing Models

Use {performance} to compare multiple models:

```{r compare-models}
model1 <- lm(mpg ~ wt, data = mtcars)
model2 <- lm(mpg ~ wt + hp, data = mtcars)
model3 <- lm(mpg ~ wt + hp + qsec, data = mtcars)

compare_performance(model1, model2, model3)
```

## Feature Engineering for Linear Regression

- Handling categorical variables: dummy coding
- Interaction terms
- Polynomial terms for non-linear relationships

```{r feature-engineering}
# Example of interaction term
lm(mpg ~ wt * hp, data = mtcars) %>% 
  tidy()
```

## Regularization Techniques

- Ridge Regression (L2)
- Lasso Regression (L1)
- Elastic Net (combination of L1 and L2)

These techniques help prevent overfitting and handle multicollinearity.

## Column-wise Models

Applying linear regression to matrix data:

```{r column-wise-models}
a <- matrix(rnorm(20), nrow = 10)
b <- a + rnorm(length(a))
result <- lm(b ~ a)

tidy(result)
```

## Key Takeaways

1. Linear regression is a powerful tool for modeling relationships
2. Use `tidy()` and `glance()` for easy model summary extraction
3. Visualize coefficients and their confidence intervals
4. Use `augment()` to add model predictions to your data
5. Always check diagnostic plots for model assumptions
6. Be aware of influential points using leverage and Cook's distance plots
7. Consider feature engineering and regularization for complex relationships
8. Use tools like {performance} for comprehensive model diagnostics

## References

1. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning.
2. Wickham, H., & Grolemund, G. (2016). R for Data Science.
3. Kuhn, M., & Silge, J. (2022). Tidy Modeling with R.
4. R Core Team (2021). R: A language and environment for statistical computing.