---
title: "Class 4 - ML in genomics: Linear Regression"
subtitle: "Predicting Dependency Scores"
author: "Renan Escalante"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 3)
library(tidyverse)
library(tidymodels)
library(broom)
library(ggplot2)
library(performance)
library(here)
```

```{r load_data, include=FALSE}
# Load the data
dependency_data <- read_csv(here::here("class_4_linear_regression/depmap/dependency_score_to_predict.csv"))

# Rename columns for easier use
dependency_data <- dependency_data %>%
  rename(
    SOX10 = `SOX10 (6663)`,
    MPPE1 = `MPPE1 (65258)`,
    CTXN1 = `CTXN1 (404217)`,
    dep_score = crispr_dep_map_public_24q2_score_chronos
  )
```

## Introduction to Linear Regression

- Fundamental statistical technique for modeling relationships
- Predicts a continuous outcome variable based on one or more predictor variables
- Assumes a linear relationship between predictors and outcome



## Simple Linear Regression

- One predictor variable (X) and one outcome variable (Y)
- Equation: Y = β0 + β1X + ε
  - β0: Y-intercept
  - β1: Slope
  - ε: Error term

```{r simple-linear-regression-1, echo=FALSE, fig.width=6, fig.height=3.5}
set.seed(123)
x <- seq(1, 10, length.out = 100)
y <- 2 + 3*x + rnorm(100, sd = 2)
data <- tibble(x = x, y = y)

ggplot(data, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Simple Linear Regression Example")
```

## Simple Linear Regression

In this example, we'll predict dependency scores using gene expression data.
Dependency scores are generated by a CRISPR screen where a negative number means cells die

```{r simple-linear-regression, echo=FALSE, fig.width=6, fig.height=3.5}
ggplot(dependency_data, aes(x = SOX10, y = dep_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Simple Linear Regression Example",
       x = "SOX10 Expression",
       y = "Dependency Score")
```

## Multiple Linear Regression

- Multiple predictor variables (X1, X2, ..., Xn)
- Equation: Y = β0 + β1X1 + β2X2 + ... + βnXn + ε

In our case: Dependency Score = β0 + β1(SOX10) + β2(MPPE1) + β3(CTXN1) + ε

## Fitting a Linear Model with tidymodels

```{r fit-linear-model}
# Create the recipe
lm_rec <- recipe(dep_score ~ SOX10 + MPPE1 + CTXN1, data = dependency_data) %>% 
  step_normalize(all_predictors())

# Create the model specification
lm_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Create and fit the workflow
lm_fit <- workflow() %>% 
  add_recipe(lm_rec) %>% 
  add_model(lm_spec) %>%
  fit(data = dependency_data)

# View model summary
tidy(lm_fit)
glance(lm_fit)
```


```{r, include=FALSE}

# Create the model specification
lm_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Fit the model
lm_fit_2 <- lm_spec %>%
  fit(dep_score ~ SOX10 + MPPE1 + CTXN1, data = dependency_data)

# View model summary
tidy(lm_fit_2)
glance(lm_fit_2)
```

## Interpreting Coefficients

- Coefficients represent the change in Y for a one-unit change in X
- Intercept (β0): Expected dependency score when all gene expressions are zero
- For each predictor (βi): Change in dependency score for a one-unit increase in gene expression, holding other predictors constant

## Coefficient Plot

Visualizing coefficients and their confidence intervals:

```{r coefficient-plot, fig.width=6, fig.height=3.5}
d <- tidy(lm_fit, conf.int = TRUE)

ggplot(d, aes(estimate, term, xmin = conf.low, xmax = conf.high, height = 0)) +
  geom_point() +
  geom_vline(xintercept = 0, lty = 4) +
  geom_errorbarh() +
  labs(title = "Coefficient Plot", x = "Estimate", y = "Term")
```

## Model Evaluation: R-squared

- R-squared measures the proportion of variance explained by the model
- Ranges from 0 to 1 (0% to 100%)
- Higher values indicate better fit, but beware of overfitting

```{r r-squared}
glance(lm_fit) %>% select(r.squared, adj.r.squared)
```

## Model Augmentation

Adding fitted values and residuals to the original data:

```{r augment-model}
augment(lm_fit, new_data = dependency_data) %>%
  select(dep_score, .pred, .resid) %>%
  head(10)
```

## Predictions with New Data

```{r predict-new-data}
new_data <- dependency_data %>%
  head(6) %>%
  mutate(SOX10 = SOX10 + 1)

augment(lm_fit, new_data = new_data) %>%
  select(SOX10, MPPE1, CTXN1, dep_score, .pred)
```

## Visualizing Predictions with Confidence Intervals

```{r visualize-predictions, fig.width=6, fig.height=3.5}
# Simpler bivariate model for 2D plotting
lm_fit2 <- lm_spec %>%
  fit(dep_score ~ SOX10, data = dependency_data)

au <- augment(lm_fit2, new_data = dependency_data, interval = "prediction")

ggplot(au, aes(SOX10, dep_score)) +
  geom_point(alpha = 0.5) +
  geom_line(aes(y = .pred), color = "blue") +
  # geom_ribbon(aes(ymin = .pred_lower, ymax = .pred_upper), alpha = 0.2) +
  labs(title = "Predictions with Confidence Intervals",
       x = "SOX10 Expression",
       y = "Dependency Score")
```

## Residual Analysis

- Residuals: Differences between observed and predicted values
- Assumptions:
  1. Linearity
  2. Independence
  3. Homoscedasticity
  4. Normality

```{r residual-plot, echo=FALSE, fig.width=6, fig.height=3.5}
au <- augment(lm_fit, new_data = dependency_data)
ggplot(au, aes(x = .pred, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residual Plot", x = "Predicted Values", y = "Residuals")
```


## Model Diagnostics with {performance}

The {performance} package provides tools for model evaluation:

```{r performance-checks, fig.width=8, fig.height=4}
lm_fit %>% 
  extract_fit_engine() %>% 
  check_model()
```

## Comparing Models

Use {performance} to compare multiple models:

```{r compare-models}
model1 <- lm_spec %>% fit(dep_score ~ SOX10, data = dependency_data)
model2 <- lm_spec %>% fit(dep_score ~ SOX10 + MPPE1, data = dependency_data)
model3 <- lm_spec %>% fit(dep_score ~ MPPE1 + CTXN1, data = dependency_data)

compare_performance(model1, model2, model3)
```

## Feature Engineering for Linear Regression

- Handling categorical variables: dummy coding
- Interaction terms
- Polynomial terms for non-linear relationships

```{r feature-engineering}
# Example of interaction term
lm_spec %>%
  fit(dep_score ~ SOX10 * MPPE1, data = dependency_data) %>%
  tidy()
```

## Key Takeaways

1. Linear regression is a powerful tool for modeling relationships in genomic data
2. Use `tidy()` and `glance()` for easy model summary extraction
3. Visualize coefficients and their confidence intervals
4. Use `augment()` to add model predictions to your data
5. Always check diagnostic plots for model assumptions
6. Be aware of influential points using leverage and Cook's distance plots
7. Consider feature engineering and regularization for complex relationships
8. Use tools like {performance} for comprehensive model diagnostics

## References

1. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning.
2. Wickham, H., & Grolemund, G. (2016). R for Data Science.
3. Kuhn, M., & Silge, J. (2022). Tidy Modeling with R.
4. R Core Team (2021). R: A language and environment for statistical computing.