---
title: "Class 5 - ML in genomics: Introduction to Logistic Regression"
author: "Renan Escalante"
date: "`r Sys.Date()`"
output: 
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 10, fig.height = 5)
library(tidyverse)
library(tidymodels)
library(broom)
```

## 1. Introduction to Logistic Regression

### What is Logistic Regression?

- A statistical method for analyzing a dataset with one or more independent variables that determine an outcome
- Used when the outcome is binary (0/1, Yes/No, True/False)
- Predicts the probability of an observation belonging to a particular category

### Comparison with Linear Regression

- Linear regression: predicts continuous outcomes
- Logistic regression: predicts categorical outcomes (usually binary)
- Both are generalized linear models (GLMs)

## 2. Applications in Genomics and Bioinformatics

- Predicting gene function
- Classifying genetic variants (e.g., pathogenic vs. benign)
- Identifying disease risk factors
- Analyzing gene expression data
- Predicting protein-protein interactions

## 3. Fundamentals of Logistic Regression

### Odds and Log Odds

- Odds: ratio of the probability of success to the probability of failure
  - Odds = P(success) / P(failure) = P(success) / (1 - P(success))
- Log odds (logit): natural logarithm of the odds
  - Log odds = ln(odds) = ln(P(success) / (1 - P(success)))

### Probability vs. Odds

- Probability: ranges from 0 to 1
- Odds: ranges from 0 to infinity
- Relationship: odds = probability / (1 - probability)

## 4. The Logit Function

- Logit function: transforms probability to log odds
- Formula: logit(p) = ln(p / (1 - p))
- Inverse logit (sigmoid) function: transforms log odds to probability
- Formula: p = 1 / (1 + e^(-logit(p)))

## 5. Logistic Regression Model

### The Logistic Function (Sigmoid Curve)

```{r sigmoid}
sigmoid <- function(x) {
  1 / (1 + exp(-x))
}

ggplot(data.frame(x = c(-6, 6)), aes(x)) +
  stat_function(fun = sigmoid) +
  labs(x = "Log odds", y = "Probability") +
  ggtitle("Sigmoid (Logistic) Function") +
  theme_minimal(base_size = 16)
```

## 6. Interpreting the Logistic Regression Equation

- General form: log(p / (1 - p)) = β₀ + β₁X₁ + β₂X₂ + ... + βₖXₖ
- p: probability of the outcome
- β₀: intercept
- β₁, β₂, ..., βₖ: coefficients for predictors X₁, X₂, ..., Xₖ

## 7. Coefficients and Their Meaning

- Intercept (β₀): log odds when all predictors are zero
- Coefficients (β₁, β₂, ..., βₖ):
  - Change in log odds for a one-unit increase in the predictor
  - Exponentiated coefficient: odds ratio for a one-unit increase in the predictor

## 8. Fitting the Logistic Regression Model

### Maximum Likelihood Estimation

- Goal: find coefficients that maximize the likelihood of observing the data
- Iterative process: no closed-form solution like in linear regression
- Optimization algorithms: Newton-Raphson, gradient descent

## 9. Comparison with Least Squares Method

- Linear regression: minimizes sum of squared residuals
- Logistic regression: maximizes likelihood function
- Both aim to find the best-fitting model, but use different criteria

## 10. Iterative Process of Finding the Best Fit

1. Start with initial guesses for coefficients
2. Calculate predicted probabilities
3. Compute the likelihood of the observed data
4. Adjust coefficients to increase likelihood
5. Repeat steps 2-4 until convergence

## 11. Evaluating Logistic Regression Models

### Confusion Matrix

```{r confusion_matrix}
set.seed(123)
n <- 100
data <- tibble(
  x = rnorm(n),
  y = factor(rbinom(n, 1, sigmoid(1 + 2 * x)))
)

log_reg_spec <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_reg_fit <- log_reg_spec %>% 
  fit(y ~ x, data = data)

predictions <- predict(log_reg_fit, new_data = data)

logistic_regression_df <- data %>% dplyr::mutate(pred_class = predictions$.pred_class)

conf_mat_df <- conf_mat(logistic_regression_df, truth = y, estimate = pred_class)

autoplot(conf_mat_df, type = "heatmap") +
  theme_minimal(base_size = 16)
```

## 12. Sensitivity and Specificity

- Sensitivity (True Positive Rate): proportion of actual positives correctly identified
- Specificity (True Negative Rate): proportion of actual negatives correctly identified

```{r sens_spec}
sens_spec <- conf_mat_df %>%
  summary() %>%
  select(-.estimator) %>%
  filter(.metric %in% c("sens", "spec"))

knitr::kable(sens_spec, digits = 3)
```

## 13. ROC (Receiver Operating Characteristic) Curves

```{r, include=FALSE}
image_dir <- here::here("class_5_logistic_regression/images_logistic_regression/")
image_files <- list.files(path = image_dir, 
                          pattern = "\\.jpg$|\\.png$|\\.gif$|\\.jpeg$", 
                          full.names = TRUE)

# Check if there are at least two images
if (length(image_files) >= 2) {
  specific_image_1 <- image_files[1]
  specific_image_2 <- image_files[2]
} else {
  specific_image_1 <- NULL
  specific_image_2 <- NULL
}
```


```{r,  out.width="50%", echo=FALSE}
# Display images side by side if available
if (!is.null(specific_image_1) && !is.null(specific_image_2)) {
  knitr::include_graphics(c(specific_image_1, specific_image_2))
} else {
  cat("Not enough images available to display.")
}

```


```{r roc_curve, include=FALSE}
predictions_prob <- predict(log_reg_fit, new_data = data, type = "prob")

logistic_regression_roc <- bind_cols(data,
                                     predictions_prob)

roc_curve_df <- roc_curve(logistic_regression_roc, truth = y, .pred_1)

autoplot(roc_curve_df) +
  theme_minimal(base_size = 16)
```

## 14. AUC (Area Under the Curve)

- Measures the overall performance of the classifier
- Ranges from 0.5 (random guessing) to 1 (perfect classification)
- Interpretation:
  - 0.5-0.7: Poor
  - 0.7-0.8: Acceptable
  - 0.8-0.9: Excellent
  - 0.9-1.0: Outstanding

```{r auc}
auc_value <- roc_auc(logistic_regression_roc, truth = y, .pred_1, estimator = "binary")
knitr::kable(auc_value, digits = 3)
```

## 15. Advanced Topics in Logistic Regression

### Multiple Logistic Regression

- Uses multiple predictors to model the outcome
- Formula: log(p / (1 - p)) = β₀ + β₁X₁ + β₂X₂ + ... + βₖXₖ
- Allows for controlling confounding variables

## 16. Categorical Predictors in Logistic Regression

- Can include categorical variables as predictors
- Use dummy coding or contrast coding
- Interpretation: effect of each category compared to the reference category

## 17. Interactions in Logistic Regression Models

- Allows for the effect of one predictor to depend on the value of another
- Can capture complex relationships between predictors
- Formula: log(p / (1 - p)) = β₀ + β₁X₁ + β₂X₂ + β₃(X₁ * X₂)

## 18. Practical Applications in Genomics

### Example: Predicting Gene Function

- Use gene features (e.g., expression levels, sequence motifs) to predict function
- Binary outcome: gene has specific function (Yes/No)
- Predictors: various genomic and proteomic data

### Example: Classifying Genetic Variants

- Predict pathogenicity of genetic variants
- Binary outcome: pathogenic vs. benign
- Predictors: conservation scores, functional impact predictions, allele frequencies

## 19. Limitations and Alternatives

### Assumptions of Logistic Regression

1. Binary or ordinal outcome variable
2. Independence of observations
3. Little or no multicollinearity among predictors
4. Linearity of independent variables and log odds
5. Large sample size

### When to Use Other Classification Methods

- Non-linear relationships: Decision trees, Random Forests
- High-dimensional data: Regularized regression (Lasso, Ridge)
- Complex patterns: Support Vector Machines, Neural Networks

## 20. Hands-on Exercise: Implementing Logistic Regression with tidymodels

### Data Preparation

```{r data_prep, echo=TRUE}
set.seed(123)
n <- 1000
gene_data <- tibble(
  gene_expression = rnorm(n),
  mutation_count = rpois(n, lambda = 2),
  is_disease_gene = factor(rbinom(n, 1, sigmoid(1 + 0.5 * gene_expression + 0.3 * mutation_count)))
)

head(gene_data)
```

## 21. Model Fitting Using tidymodels

```{r model_fitting, echo=TRUE}
log_reg_spec <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_reg_fit <- log_reg_spec %>% 
  fit(is_disease_gene ~ gene_expression + mutation_count, data = gene_data)

tidy(log_reg_fit) %>% 
  knitr::kable(digits = 3)
```

## 22. Interpretation of Results

- Intercept: log odds of being a disease gene when gene expression and mutation count are zero
- Gene expression coefficient: increase in log odds for one unit increase in gene expression
- Mutation count coefficient: increase in log odds for one additional mutation

```{r odds_ratios, echo=TRUE}
tidy(log_reg_fit, exponentiate = TRUE) %>% 
  knitr::kable(digits = 3)
```

Interpretation: 
- For each unit increase in gene expression, the odds of being a disease gene increase by a factor of the exponentiated coefficient
- For each additional mutation, the odds of being a disease gene increase by a factor of the exponentiated coefficient

## 23. Model Evaluation and Visualization

```{r model_eval, echo=TRUE}
# Predictions
predictions <- predict(log_reg_fit, new_data = gene_data, type = "prob") %>% 
  bind_cols(gene_data)

# ROC curve
roc_curve <- roc_curve(predictions, truth = is_disease_gene, .pred_1)
roc_auc <- roc_auc(predictions, truth = is_disease_gene, .pred_1)

# Plot ROC curve
autoplot(roc_curve) +
  labs(title = paste("ROC Curve (AUC =", round(roc_auc$.estimate, 3), ")")) +
  theme_minimal(base_size = 16)

# Confusion matrix (using 0.5 as threshold)
predictions_class <- predict(log_reg_fit, new_data = gene_data, type = "class")

prediction_class_df <- bind_cols(gene_data, predictions_class)


conf_mat_df <- conf_mat(prediction_class_df, truth = is_disease_gene, estimate = .pred_class)

# Plot confusion matrix
autoplot(conf_mat_df, type = "heatmap") +
  theme_minimal(base_size = 16)
```

```{r}
# Calculate accuracy
accuracy <- accuracy(prediction_class_df, truth = is_disease_gene, estimate = .pred_class)
cat("Accuracy:", round(accuracy$.estimate, 3))
```



```{r}

```

