---
title: "Class 6 - ML in Genomics: Decision Trees to Random Forests"
author: "Renan Escalante"
date: "2024-10-17"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidymodels)
library(palmerpenguins)
library(vip)
```

# Introduction to Decision Trees

--

- Tree-like structure: root node, internal nodes, leaves

--

- Decisions made at each node based on feature values

--

- Classification process: traverse from root to leaf

--

- Recursive partitioning: splitting data into subsets

---

# Gini Impurity

--

- Measure of node purity in classification trees

--

- Used to choose the best split at each node

--

- Formula: $Gini = 1 - \sum_{i=1}^{c} p_i^2$

  where $c$ is the number of classes and $p_i$ is the proportion of samples in class $i$

--

- Lower Gini impurity indicates better split

---

# Gini Impurity Example

Consider a node with 10 samples:
- 6 samples of class A
- 4 samples of class B

--

Gini impurity calculation:

$Gini = 1 - (\frac{6}{10})^2 - (\frac{4}{10})^2$

$Gini = 1 - 0.36 - 0.16 = 0.48$

---

# Advantages of Decision Trees

--

- Interpretability: Easy to understand and explain

--

- Handle non-linear relationships between features

--

- Can work with both numerical and categorical data

--

- No need for feature scaling

---

# Limitations of Decision Trees

--

- Tendency to overfit, especially with deep trees

--

- Instability: Small changes in data can lead to very different trees

--

- May struggle with highly imbalanced datasets

--

- Not ideal for capturing linear relationships

---

# Regression Trees

--

- Similar structure to classification trees, but predict continuous values

--

- Splitting criteria: Minimize sum of squared residuals

--

- Leaf values: Usually the mean of samples in the leaf

--

- Example use case: Predicting gene expression levels

---

# Building a Regression Tree

1. Start with all data in the root node
2. For each feature, find the best split that minimizes SSR
3. Choose the feature and split point with lowest overall SSR
4. Repeat steps 2-3 for each resulting node until stopping criteria met

---

# Pruning Regression Trees

--

- Necessary to prevent overfitting

--

- Cost-complexity pruning: Balance tree size and prediction accuracy

--

- Process:
  1. Grow a large tree
  2. Calculate cost-complexity for different tree sizes
  3. Use cross-validation to choose optimal complexity parameter
  4. Prune the tree based on the chosen parameter

---

# Evaluating Regression Trees

--

- Use cross-validation to estimate performance

--

- Common metrics:
  - Mean Squared Error (MSE)
  - Root Mean Squared Error (RMSE)
  - R-squared (RÂ²)
  - Mean Absolute Error (MAE)

--

- Compare performance with other models (e.g., linear regression)

---

# Hands-on: Penguin Classification

Let's start exploring the Palmer Penguins dataset for classification:

```{r}
data(penguins)
head(penguins)
```

---

# Data Understanding

Let's examine the structure of the penguins dataset:

```{r}
str(penguins)
```

--

Features for classification:
- bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g
- island, sex

Target variable:
- species

---

# Data Summary

We can use `skimr::skim()` to get a comprehensive summary:

```{r}
library(skimr)
skim(penguins)
```

---

# Visualization of Relationships

Let's visualize relationships between features:

```{r, fig.height=6, fig.width=10}
library(GGally)
penguins %>%
  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%
  ggpairs(aes(color = species, alpha = 0.8))
```

---

# Data Preparation

Remove rows with missing values:

```{r}
penguins_clean <- penguins %>%
  drop_na()

print(paste("Rows before:", nrow(penguins)))
print(paste("Rows after:", nrow(penguins_clean)))
```

---

# Train-Test Split

Split the data into training and testing sets:

```{r}
set.seed(123)
penguin_split <- initial_split(penguins_clean, prop = 0.75, strata = species)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)

print(paste("Training set size:", nrow(penguin_train)))
print(paste("Testing set size:", nrow(penguin_test)))
```

---

# Recipe for Classification

Create a recipe for data preprocessing:

```{r}
penguin_recipe <- recipe(species ~ ., data = penguin_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors(), -all_outcomes()) %>%
  step_zv(all_predictors())

# Print the recipe
penguin_recipe
```

---

# Cross-Validation Folds

Create a validation set using k-fold cross-validation:

```{r}
set.seed(234)
penguin_folds <- vfold_cv(penguin_train, v = 5, strata = species)
penguin_folds
```

---

# Decision Tree Model Specification

Specify a decision tree model for classification:

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_spec
```

---

# Random Forest Model Specification

Specify a random forest model for classification:

```{r}
rf_spec <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_spec
```

---

# Creating Workflows

Create workflows for both classification models:

```{r}
tree_wflow <- workflow() %>%
  add_recipe(penguin_recipe) %>%
  add_model(tree_spec)

rf_wflow <- workflow() %>%
  add_recipe(penguin_recipe) %>%
  add_model(rf_spec)

# Print workflows
tree_wflow
rf_wflow
```

---

# Fitting Classification Models

Let's fit our models to the resamples and collect performance metrics:

```{r}
tree_res <- tree_wflow %>%
  fit_resamples(
    resamples = penguin_folds,
    metrics = metric_set(accuracy, roc_auc, kap),
    control = control_resamples(save_pred = TRUE)
  )

rf_res <- rf_wflow %>%
  fit_resamples(
    resamples = penguin_folds,
    metrics = metric_set(accuracy, roc_auc, kap),
    control = control_resamples(save_pred = TRUE)
  )
```

---

# Comparing Classification Models

Compare the performance of decision tree and random forest:

```{r}
bind_rows(
  collect_metrics(tree_res) %>% mutate(model = "Decision Tree"),
  collect_metrics(rf_res) %>% mutate(model = "Random Forest")
) %>%
  select(model, .metric, mean, std_err) %>%
  pivot_wider(names_from = .metric, values_from = c(mean, std_err))
```

---

# Variable Importance (Random Forest)

Create a variable importance plot for the random forest model:

```{r}
rf_wflow %>%
  fit(penguin_train) %>%
  pull_workflow_fit() %>%
  vip(num_features = 10)
```

---

# Final Model Evaluation (Classification)

Fit the best performing model (random forest) to the entire training set and evaluate on the test set:

```{r}
rf_final <- rf_wflow %>%
  last_fit(penguin_split)

collect_metrics(rf_final)
```

---

# Confusion Matrix (Classification)

Create a confusion matrix for the classification predictions on the test set:

```{r}
rf_final %>%
  collect_predictions() %>%
  conf_mat(truth = species, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

---

# Regression: Data Preparation

Create a new recipe for regression, using body_mass_g as the target variable:

```{r}
penguin_reg_recipe <- recipe(body_mass_g ~ ., data = penguin_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

# Print the recipe
penguin_reg_recipe
```

---

# Regression: Model Specification

Specify regression tree and random forest models:

```{r}
reg_tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("regression")

reg_rf_spec <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")

# Print specifications
reg_tree_spec
reg_rf_spec
```

---

# Regression: Creating Workflows

Create workflows for both regression models:

```{r}
reg_tree_wflow <- workflow() %>%
  add_recipe(penguin_reg_recipe) %>%
  add_model(reg_tree_spec)

reg_rf_wflow <- workflow() %>%
  add_recipe(penguin_reg_recipe) %>%
  add_model(reg_rf_spec)

# Print workflows
reg_tree_wflow
reg_rf_wflow
```

---

# Fitting Regression Models

Fit the regression models to the resamples and collect performance metrics:

```{r}
reg_tree_res <- reg_tree_wflow %>%
  fit_resamples(
    resamples = penguin_folds,
    metrics = metric_set(rmse, rsq, mae),
    control = control_resamples(save_pred = TRUE)
  )

reg_rf_res <- reg_rf_wflow %>%
  fit_resamples(
    resamples = penguin_folds,
    metrics = metric_set(rmse, rsq, mae),
    control = control_resamples(save_pred = TRUE)
  )
```

---

# Comparing Regression Models

Compare the performance of regression tree and random forest:

```{r}
bind_rows(
  collect_metrics(reg_tree_res) %>% mutate(model = "Regression Tree"),
  collect_metrics(reg_rf_res) %>% mutate(model = "Random Forest")
) %>%
  select(model, .metric, mean, std_err) %>%
  pivot_wider(names_from = .metric, values_from = c(mean, std_err))
```

